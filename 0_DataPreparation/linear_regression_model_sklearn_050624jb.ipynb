{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6999 entries, 0 to 6998\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Datum                6999 non-null   object \n",
      " 1   Bewoelkung           6999 non-null   float64\n",
      " 2   Temperatur           6999 non-null   float64\n",
      " 3   Windgeschwindigkeit  6999 non-null   float64\n",
      " 4   Wettercode           6999 non-null   float64\n",
      " 5   KielerWoche          6999 non-null   float64\n",
      " 6   Warengruppe          6999 non-null   float64\n",
      " 7   Umsatz               6999 non-null   float64\n",
      " 8   Feiertage            6999 non-null   float64\n",
      " 9   Ferientage           6999 non-null   float64\n",
      " 10  Wetterklasse         6999 non-null   int64  \n",
      " 11  Niederschlag         6999 non-null   float64\n",
      " 12  year                 6999 non-null   int64  \n",
      " 13  month                6999 non-null   int64  \n",
      " 14  week                 6999 non-null   int64  \n",
      " 15  weekday              6999 non-null   int64  \n",
      " 16  day_month            6999 non-null   int64  \n",
      " 17  season_str           6999 non-null   object \n",
      " 18  season               6999 non-null   int64  \n",
      "dtypes: float64(10), int64(7), object(2)\n",
      "memory usage: 1.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datum</th>\n",
       "      <th>Bewoelkung</th>\n",
       "      <th>Temperatur</th>\n",
       "      <th>Windgeschwindigkeit</th>\n",
       "      <th>Wettercode</th>\n",
       "      <th>KielerWoche</th>\n",
       "      <th>Warengruppe</th>\n",
       "      <th>Umsatz</th>\n",
       "      <th>Feiertage</th>\n",
       "      <th>Ferientage</th>\n",
       "      <th>Wetterklasse</th>\n",
       "      <th>Niederschlag</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day_month</th>\n",
       "      <th>season_str</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.828353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>535.856285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>201.198426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65.890169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>317.475875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.7875</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.541340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.7875</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>616.358562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.7875</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>348.770346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.7875</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.822977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.7875</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>324.965348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>summer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6999 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Datum  Bewoelkung  Temperatur  Windgeschwindigkeit  Wettercode  \\\n",
       "0     2013-07-01         6.0     17.8375                 15.0        20.0   \n",
       "1     2013-07-01         6.0     17.8375                 15.0        20.0   \n",
       "2     2013-07-01         6.0     17.8375                 15.0        20.0   \n",
       "3     2013-07-01         6.0     17.8375                 15.0        20.0   \n",
       "4     2013-07-01         6.0     17.8375                 15.0        20.0   \n",
       "...          ...         ...         ...                  ...         ...   \n",
       "6994  2018-07-30         6.0     27.7875                 10.0        21.0   \n",
       "6995  2018-07-30         6.0     27.7875                 10.0        21.0   \n",
       "6996  2018-07-30         6.0     27.7875                 10.0        21.0   \n",
       "6997  2018-07-30         6.0     27.7875                 10.0        21.0   \n",
       "6998  2018-07-30         6.0     27.7875                 10.0        21.0   \n",
       "\n",
       "      KielerWoche  Warengruppe      Umsatz  Feiertage  Ferientage  \\\n",
       "0             0.0          1.0  148.828353        0.0         1.0   \n",
       "1             0.0          2.0  535.856285        0.0         1.0   \n",
       "2             0.0          3.0  201.198426        0.0         1.0   \n",
       "3             0.0          4.0   65.890169        0.0         1.0   \n",
       "4             0.0          5.0  317.475875        0.0         1.0   \n",
       "...           ...          ...         ...        ...         ...   \n",
       "6994          0.0          1.0  148.541340        0.0         1.0   \n",
       "6995          0.0          2.0  616.358562        0.0         1.0   \n",
       "6996          0.0          3.0  348.770346        0.0         1.0   \n",
       "6997          0.0          4.0   71.822977        0.0         1.0   \n",
       "6998          0.0          5.0  324.965348        0.0         1.0   \n",
       "\n",
       "      Wetterklasse  Niederschlag  year  month  week  weekday  day_month  \\\n",
       "0                6           0.3  2013      7    27        0          1   \n",
       "1                6           0.3  2013      7    27        0          1   \n",
       "2                6           0.3  2013      7    27        0          1   \n",
       "3                6           0.3  2013      7    27        0          1   \n",
       "4                6           0.3  2013      7    27        0          1   \n",
       "...            ...           ...   ...    ...   ...      ...        ...   \n",
       "6994             6           0.0  2018      7    31        0         30   \n",
       "6995             6           0.0  2018      7    31        0         30   \n",
       "6996             6           0.0  2018      7    31        0         30   \n",
       "6997             6           0.0  2018      7    31        0         30   \n",
       "6998             6           0.0  2018      7    31        0         30   \n",
       "\n",
       "     season_str  season  \n",
       "0        summer       2  \n",
       "1        summer       2  \n",
       "2        summer       2  \n",
       "3        summer       2  \n",
       "4        summer       2  \n",
       "...         ...     ...  \n",
       "6994     summer       2  \n",
       "6995     summer       2  \n",
       "6996     summer       2  \n",
       "6997     summer       2  \n",
       "6998     summer       2  \n",
       "\n",
       "[6999 rows x 19 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "# url = \"../sourcedata/cleaned_data/6_weather_parameters.csv\"\n",
    "# df = pd.read_csv(url)\n",
    "# df\n",
    "\n",
    "# Read the file and check it out\n",
    "df = pd.read_csv(r'/Users/jberndt/Desktop/2-2-opencampus/X-opencampus_ML/2_DS_and_ML/6-homework_week06/6_weather_parameters.csv')\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 19653.813325191844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# Define the column that we want to predict\n",
    "X = df.drop(['Umsatz', 'Datum', 'Wettercode', 'year', 'month', 'week', 'season_str'], axis=1)  # Exclude the columns that shall not be included\n",
    "y = df['Umsatz']\n",
    "\n",
    "# Identify numerical columns and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Add 'Wetterklasse' to categorical_cols\n",
    "categorical_cols.append('Wetterklasse')\n",
    "\n",
    "# Exclude weather code from numerical columns\n",
    "int_or_float_cols = ['Bewoelkung', 'KielerWoche', 'Warengruppe', 'Feiertage', 'Ferientage', 'day_month']\n",
    "numerical_cols = [col for col in numerical_cols if col not in int_or_float_cols]\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(cols=categorical_cols)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Combine the original numerical columns with the encoded categorical columns\n",
    "X_final = pd.concat([X[numerical_cols], X_encoded], axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model is not performing well, as indicated by the high Mean Squared Error (MSE) value of 19653.813325191844. This error measure quantifies the average difference between the true and predicted values, so a large MSE indicates that the model's predictions are significantly different from the actual values. There are several possible reasons for a high MSE value, including:\n",
    "\n",
    "1. **Inadequate model**: The chosen model (Linear Regression) might not be the best model for the given data. You could try other regression models like Ridge Regression, Lasso Regression, Elastic Net, or even machine learning algorithms like Random Forest Regressor or Gradient Boosting Regressor.\n",
    "\n",
    "2. **Feature engineering**: The current feature set might not be sufficient to capture the relationship between the input and output variables. You could consider adding new features, transforming the existing features, or selecting different features using techniques like Principal Component Analysis (PCA) or Recursive Feature Elimination (RFE).\n",
    "\n",
    "3. **Data quality**: The data might contain errors, missing values, or outliers that could negatively impact the model's performance. You could consider cleaning the data, removing outliers, or handling missing values using appropriate techniques.\n",
    "\n",
    "4. **Model hyperparameters**: The current model hyperparameters might not be optimal. You could experiment with different hyperparameters, such as regularization strength or learning rate, to improve the model's performance.\n",
    "\n",
    "To improve the model's performance, you could try the following steps:\n",
    "\n",
    "1. **Experiment with different models**: Try using other regression models or machine learning algorithms and compare their performance using various evaluation metrics.\n",
    "\n",
    "2. **Feature engineering**: Investigate the relationship between the input features and the output variable and engineer new features or transform the existing features to improve the model's performance.\n",
    "\n",
    "3. **Data cleaning**: Clean the data by removing errors, handling missing values, and removing outliers.\n",
    "\n",
    "4. **Model hyperparameter tuning**: Experiment with different hyperparameter values to find the optimal settings for the chosen model.\n",
    "\n",
    "5. **Ensemble methods**: Use ensemble methods like Random Forest Regressor, Gradient Boosting Regressor, or bagging to combine multiple models and improve the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model performance and reduce the Mean Squared Error (MSE), you can consider the following strategies:\n",
    "\n",
    "1. Feature Engineering:\n",
    "- Explore additional features that may have predictive power for the target variable 'Umsatz'.\n",
    "- Consider creating new features based on domain knowledge or interactions between existing features.\n",
    "- Check for multicollinearity among the features and remove redundant or highly correlated features.\n",
    "\n",
    "2. Data Preprocessing:\n",
    "- Standardize or normalize numerical features to ensure they are on a similar scale.\n",
    "- Handle missing values in the dataset using appropriate techniques such as imputation or removal.\n",
    "- Encode categorical variables effectively, considering different encoding methods like target encoding or frequency encoding.\n",
    "\n",
    "3. Model Selection and Tuning:\n",
    "- Experiment with different regression models such as Ridge Regression, Lasso Regression, or Decision Trees to see if they perform better than Linear Regression.\n",
    "- Perform hyperparameter tuning using techniques like GridSearchCV or RandomizedSearchCV to find the optimal parameters for the chosen model.\n",
    "- Consider ensemble methods like Random Forest or Gradient Boosting for potentially improved performance.\n",
    "\n",
    "4. Cross-Validation:\n",
    "- Implement cross-validation techniques like k-fold cross-validation to assess the model's generalization performance and reduce overfitting.\n",
    "\n",
    "5. Regularization:\n",
    "- Apply regularization techniques like L1 (Lasso) or L2 (Ridge) regularization to prevent overfitting and improve model generalization.\n",
    "\n",
    "By incorporating these strategies and fine-tuning your model, you can work towards improving its performance and reducing the Mean Squared Error. Remember to evaluate the model's performance on both the training and testing datasets to ensure it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "                      Bewoelkung  Temperatur  Windgeschwindigkeit  Wettercode  \\\n",
      "Bewoelkung             1.000000    0.293302             0.003829    0.524281   \n",
      "Temperatur             0.293302    1.000000             0.057264    0.085217   \n",
      "Windgeschwindigkeit    0.003829    0.057264             1.000000    0.120354   \n",
      "Wettercode             0.524281    0.085217             0.120354    1.000000   \n",
      "KielerWoche            0.004290    0.166036             0.016537    0.067041   \n",
      "Warengruppe            0.020495    0.042958             0.009703    0.009581   \n",
      "Umsatz                 0.063891    0.224926             0.027708    0.009880   \n",
      "Feiertage              0.031130    0.035565             0.000431    0.005797   \n",
      "Ferientage             0.043719    0.198472             0.043579    0.035105   \n",
      "Wetterklasse           0.538271    0.074773             0.198274    0.741259   \n",
      "Niederschlag           0.226376    0.043528             0.139073    0.438680   \n",
      "year                   0.146653    0.013082             0.026667    0.096132   \n",
      "month                  0.043368    0.234905             0.081997    0.039244   \n",
      "week                   0.048406    0.233214             0.077020    0.039164   \n",
      "weekday                0.002754    0.012033             0.017535    0.012042   \n",
      "day_month              0.004918    0.041736             0.017105    0.004863   \n",
      "season                 0.225231    0.484247             0.139730    0.111482   \n",
      "\n",
      "                     KielerWoche  Warengruppe    Umsatz  Feiertage  \\\n",
      "Bewoelkung              0.004290     0.020495  0.063891   0.031130   \n",
      "Temperatur              0.166036     0.042958  0.224926   0.035565   \n",
      "Windgeschwindigkeit     0.016537     0.009703  0.027708   0.000431   \n",
      "Wettercode              0.067041     0.009581  0.009880   0.005797   \n",
      "KielerWoche             1.000000     0.010395  0.071011   0.021873   \n",
      "Warengruppe             0.010395     1.000000  0.056812   0.007344   \n",
      "Umsatz                  0.071011     0.056812  1.000000   0.043130   \n",
      "Feiertage               0.021873     0.007344  0.043130   1.000000   \n",
      "Ferientage              0.065900     0.014833  0.101038   0.059703   \n",
      "Wetterklasse            0.017080     0.005761  0.003389   0.015605   \n",
      "Niederschlag            0.029940     0.005541  0.005300   0.013755   \n",
      "year                    0.053453     0.032931  0.061584   0.003750   \n",
      "month                   0.024608     0.087179  0.014124   0.011294   \n",
      "week                    0.015551     0.086335  0.004584   0.013109   \n",
      "weekday                 0.031697     0.002426  0.144038   0.006547   \n",
      "day_month               0.120451     0.005914  0.036697   0.024308   \n",
      "season                  0.082912     0.055061  0.115606   0.101449   \n",
      "\n",
      "                     Ferientage  Wetterklasse  Niederschlag      year  \\\n",
      "Bewoelkung             0.043719      0.538271      0.226376  0.146653   \n",
      "Temperatur             0.198472      0.074773      0.043528  0.013082   \n",
      "Windgeschwindigkeit    0.043579      0.198274      0.139073  0.026667   \n",
      "Wettercode             0.035105      0.741259      0.438680  0.096132   \n",
      "KielerWoche            0.065900      0.017080      0.029940  0.053453   \n",
      "Warengruppe            0.014833      0.005761      0.005541  0.032931   \n",
      "Umsatz                 0.101038      0.003389      0.005300  0.061584   \n",
      "Feiertage              0.059703      0.015605      0.013755  0.003750   \n",
      "Ferientage             1.000000      0.004123      0.021929  0.018444   \n",
      "Wetterklasse           0.004123      1.000000      0.316849  0.120487   \n",
      "Niederschlag           0.021929      0.316849      1.000000  0.083074   \n",
      "year                   0.018444      0.120487      0.083074  1.000000   \n",
      "month                  0.093771      0.036573      0.071334  0.314123   \n",
      "week                   0.077505      0.032206      0.076669  0.309683   \n",
      "weekday                0.210304      0.025910      0.012161  0.012713   \n",
      "day_month              0.031329      0.018775      0.044322  0.023331   \n",
      "season                 0.065202      0.090446      0.050087  0.159291   \n",
      "\n",
      "                        month      week   weekday  day_month    season  \n",
      "Bewoelkung           0.043368  0.048406  0.002754   0.004918  0.225231  \n",
      "Temperatur           0.234905  0.233214  0.012033   0.041736  0.484247  \n",
      "Windgeschwindigkeit  0.081997  0.077020  0.017535   0.017105  0.139730  \n",
      "Wettercode           0.039244  0.039164  0.012042   0.004863  0.111482  \n",
      "KielerWoche          0.024608  0.015551  0.031697   0.120451  0.082912  \n",
      "Warengruppe          0.087179  0.086335  0.002426   0.005914  0.055061  \n",
      "Umsatz               0.014124  0.004584  0.144038   0.036697  0.115606  \n",
      "Feiertage            0.011294  0.013109  0.006547   0.024308  0.101449  \n",
      "Ferientage           0.093771  0.077505  0.210304   0.031329  0.065202  \n",
      "Wetterklasse         0.036573  0.032206  0.025910   0.018775  0.090446  \n",
      "Niederschlag         0.071334  0.076669  0.012161   0.044322  0.050087  \n",
      "year                 0.314123  0.309683  0.012713   0.023331  0.159291  \n",
      "month                1.000000  0.977381  0.020300   0.004146  0.186295  \n",
      "week                 0.977381  1.000000  0.020380   0.051898  0.183903  \n",
      "weekday              0.020300  0.020380  1.000000   0.035424  0.015417  \n",
      "day_month            0.004146  0.051898  0.035424   1.000000  0.035397  \n",
      "season               0.186295  0.183903  0.015417   0.035397  1.000000  \n",
      "\n",
      "Features selected using SelectKBest with R2 score:\n",
      "             Temperatur  KielerWoche  Ferientage  weekday  season\n",
      "Temperatur          NaN          NaN         NaN      NaN     NaN\n",
      "KielerWoche         NaN          NaN         NaN      NaN     NaN\n",
      "Ferientage          NaN          NaN         NaN      NaN     NaN\n",
      "weekday             NaN          NaN         NaN      NaN     NaN\n",
      "season              NaN          NaN         NaN      NaN     NaN\n",
      "\n",
      "Linear Regression Results with Selected Features:\n",
      "R2 score: 0.0764433642818958\n",
      "Mean squared error: 18315.060756938794\n",
      "Mean absolute error: 103.68964335088906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Your DataFrame\n",
    "X = df.drop(['Datum', 'season_str'], axis=1)  # Exclude the columns that are not numeric\n",
    "y = df['Umsatz']\n",
    "\n",
    "# Correlation matrix\n",
    "corr = X.corr().abs()\n",
    "print(\"Correlation matrix:\\n\", corr)\n",
    "\n",
    "# Selecting features based on correlation score\n",
    "selector = SelectKBest(score_func=f_regression, k=5)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_selected = pd.DataFrame(X.iloc[:, selector.get_support(indices=True)].T, columns=X.columns[selector.get_support(indices=True)])\n",
    "print(\"\\nFeatures selected using SelectKBest with R2 score:\")\n",
    "print(X_selected)\n",
    "\n",
    "# Linear Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Selecting the features using the indices\n",
    "selected_columns = X.columns[selector.get_support(indices=True)]\n",
    "X_train_selected = X_train[selected_columns]\n",
    "X_test_selected = X_test[selected_columns]\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = linear_regression.predict(X_test_selected)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nLinear Regression Results with Selected Features:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred))\n",
    "print(\"Mean squared error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided results, the linear regression model with the selected features has a relatively low R-squared score, indicating that only a small portion of the variance in the target variable (Umsatz) can be explained by the selected features. Additionally, the mean squared error and mean absolute error are relatively high, suggesting that the model has limited predictive power.\n",
    "\n",
    "To improve the model, you can consider the following steps:\n",
    "\n",
    "1. Check for data preprocessing and feature engineering: Ensure that the data is clean and preprocessed properly. Feature engineering techniques like normalization, scaling, or feature transformations can help improve model performance.\n",
    "\n",
    "2. Try different models: You can experiment with different regression models or machine learning algorithms to identify if there is a better model for your dataset. For instance, you can try Ridge Regression, Lasso Regression, or Elastic Net.\n",
    "\n",
    "3. Feature selection: You may also want to try different feature selection techniques like Lasso Regression with `alpha=0` (L1 regularization), or Elastic Net with a mix of L1 and L2 regularization. These methods perform both regression and feature selection in a single step, which can improve the model's performance.\n",
    "\n",
    "4. Cross-validation: You can use k-fold cross-validation to evaluate the model's performance and generalizability. This can help you identify if the model is overfitting or underfitting the data.\n",
    "\n",
    "5. Model tuning: You can try tuning the model's hyperparameters using techniques like Grid Search or Random Search to find optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Results with Selected Features:\n",
      "R2 score: 0.04972141045782785\n",
      "Mean squared error: 18844.98408692643\n",
      "Mean absolute error: 105.3104643418714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# Based on the given code and the correlation matrix, it seems that the \"Temperatur\" and \"KielerWoche\" columns have relatively high correlation coefficients with the target variable \"Umsatz.\" You might want to consider keeping these two features in your model instead of using SelectKBest with R2 score, which appears to be selecting irrelevant features.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Your DataFrame\n",
    "X = df[['Temperatur', 'KielerWoche']]  # Select the desired columns\n",
    "y = df['Umsatz']\n",
    "\n",
    "# Linear Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Selecting the features using the indices\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = linear_regression.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nLinear Regression Results with Selected Features:\")\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred))\n",
    "print(\"Mean squared error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "# This updated code selects only the 'Temperatur' and 'KielerWoche' columns for the linear regression model. These two columns have relatively high correlation with the target variable \"Umsatz\" as per the correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 score, mean squared error, and mean absolute error are all measures of how well the linear regression model fits the data. A lower R2 score indicates that the model explains a smaller percentage of the variance in the data, while a lower mean squared error and mean absolute error indicate better prediction accuracy.\n",
    "\n",
    "In this case, the updated model with 'Temperatur' and 'KielerWoche' as features has a lower R2 score (0.0497) compared to the original R2 score (0.0764). This means that the model explains a smaller percentage of the variance in the data with these two features.\n",
    "\n",
    "However, the mean squared error and mean absolute error have also decreased slightly compared to the original values. This suggests that the model with 'Temperatur' and 'KielerWoche' as features still provides some improvement in prediction accuracy compared to the full model with all features.\n",
    "\n",
    "Ultimately, the choice of which model to use depends on the specific goals and constraints of your analysis. If explaining a larger percentage of the variance in the data is a priority, you might consider keeping more features in the model. If prediction accuracy is more important, the model with 'Temperatur' and 'KielerWoche' as features could be a reasonable choice.\n",
    "\n",
    "It's also worth noting that there are other techniques, such as stepwise regression and Lasso/Ridge regression, that can be used to select and fit features in a more systematic way. These methods might provide better results in terms of model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/var/folders/df/036b7scd3f90j03plj3rnhrxnr0csc/T/ipykernel_45804/2198933905.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sorted_indices = sorted(coefs.columns, key=lambda x: abs(coefs[x][0]), reverse=True)[:10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Top 10 Features:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m sorted_indices:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# print(X_train.columns[int(index)])\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Create and fit the Ridge Regression model using the selected features\u001b[39;00m\n\u001b[1;32m     55\u001b[0m X_selected \u001b[38;5;241m=\u001b[39m X_train[X_train\u001b[38;5;241m.\u001b[39mcolumns[sorted_indices]]  \u001b[38;5;66;03m# Select the top 19 features\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensor_course/lib/python3.10/site-packages/pandas/core/indexes/base.py:5382\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5379\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5380\u001b[0m         key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m-> 5382\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5383\u001b[0m \u001b[38;5;66;03m# Because we ruled out integer above, we always get an arraylike here\u001b[39;00m\n\u001b[1;32m   5384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Import Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the column that we want to predict\n",
    "X = df.drop(['Umsatz', 'Datum', 'Wettercode', 'year', 'month', 'week', 'season_str'], axis=1)  # Exclude the columns that shall not be included\n",
    "y = df['Umsatz']\n",
    "\n",
    "# Identify numerical columns and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Add 'Wetterklasse' to categorical_cols\n",
    "categorical_cols.append('Wetterklasse')\n",
    "\n",
    "# Exclude weather code from numerical columns\n",
    "int_or_float_cols = ['Bewoelkung', 'KielerWoche', 'Warengruppe', 'Feiertage', 'Ferientage', 'day_month']\n",
    "numerical_cols = [col for col in numerical_cols if col not in int_or_float_cols]\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(cols=categorical_cols)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Combine the original numerical columns with the encoded categorical columns\n",
    "X_final = pd.concat([X[numerical_cols], X_encoded], axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Implement Feature Selection using Ridge Regression\n",
    "# Initialize the Ridge Regression model\n",
    "ridge_reg = Ridge(alpha=1)\n",
    "\n",
    "# Fit the model\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get intercept and coefficients separately\n",
    "intercept = pd.DataFrame(ridge_reg.intercept_, index=X_train.columns, columns=['Intercept'])\n",
    "coefs = pd.DataFrame(ridge_reg.coef_, index=X_train.columns, columns=['Coefficient'])\n",
    "\n",
    "# Convert columns to numeric type\n",
    "coefs = coefs.astype(float)\n",
    "\n",
    "# Sort the columns based on their absolute coefficients\n",
    "sorted_indices = sorted(coefs.columns, key=lambda x: abs(coefs[x][0]), reverse=True)[:10]\n",
    "\n",
    "# Print the column names of the top 10 features\n",
    "print(\"Top 10 Features:\")\n",
    "for index in sorted_indices:\n",
    "    print(X_train.columns[index])\n",
    "    # print(X_train.columns[int(index)])\n",
    "\n",
    "# Create and fit the Ridge Regression model using the selected features\n",
    "X_selected = X_train[X_train.columns[sorted_indices]]  # Select the top 19 features\n",
    "y_selected = y_train\n",
    "ridge_reg_selected = Ridge(alpha=1)\n",
    "ridge_reg_selected.fit(X_selected, y_selected)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = ridge_reg_selected.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
